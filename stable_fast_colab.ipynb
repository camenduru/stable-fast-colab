{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chengzeyi/stable-fast-colab/blob/main/stable_fast_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHE6H6P4g8jX"
      },
      "outputs": [],
      "source": [
        "# !pip install -q https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp310-cp310-linux_x86_64.whl\n",
        "# !pip install -q https://download.pytorch.org/whl/torchdata-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
        "# !pip install -q https://download.pytorch.org/whl/torchtext-0.16.0%2Bcpu-cp310-cp310-linux_x86_64.whl\n",
        "# !pip install -q https://download.pytorch.org/whl/cu118/torchvision-0.16.0%2Bcu118-cp310-cp310-linux_x86_64.whl\n",
        "# !pip install -q fastai==2.7.13\n",
        "\n",
        "!pip install -q https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp310-cp310-linux_x86_64.whl\n",
        "!pip install -q https://download.pytorch.org/whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl\n",
        "!pip install -q https://download.pytorch.org/whl/cu118/xformers-0.0.22.post4%2Bcu118-cp310-cp310-manylinux2014_x86_64.whl\n",
        "!pip install -q diffusers transformers accelerate\n",
        "!pip install -q https://github.com/chengzeyi/stable-fast/releases/download/v0.0.13/stable_fast-0.0.13+torch210cu118-cp310-cp310-manylinux2014_x86_64.whl\n",
        "\n",
        "# !pip install -q ninja\n",
        "# !pip install -q git+https://github.com/chengzeyi/stable-fast.git@main#egg=stable-fast\n",
        "\n",
        "# !apt -y update -qq\n",
        "!wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "%env LD_PRELOAD=/content/libtcmalloc_minimal.so.4\n",
        "\n",
        "!ldconfig /usr/lib64-nvidia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qr0CuPdg8ja"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "from diffusers import (StableDiffusionPipeline,\n",
        "                       EulerAncestralDiscreteScheduler)\n",
        "from sfast.compilers.stable_diffusion_pipeline_compiler import (\n",
        "    compile, CompilationConfig)\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    model = StableDiffusionPipeline.from_pretrained(\n",
        "        'runwayml/stable-diffusion-v1-5',\n",
        "        torch_dtype=torch.float16)\n",
        "\n",
        "    model.scheduler = EulerAncestralDiscreteScheduler.from_config(\n",
        "        model.scheduler.config)\n",
        "    model.safety_checker = None\n",
        "    model.to(torch.device('cuda'))\n",
        "    return model\n",
        "\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "config = CompilationConfig.Default()\n",
        "# xformers and Triton are suggested for achieving best performance.\n",
        "try:\n",
        "    import xformers\n",
        "    config.enable_xformers = True\n",
        "except ImportError:\n",
        "    print('xformers not installed, skip')\n",
        "try:\n",
        "    import triton\n",
        "    config.enable_triton = True\n",
        "except ImportError:\n",
        "    print('Triton not installed, skip')\n",
        "# CUDA Graph is suggested for small batch sizes and small resolutions to reduce CPU overhead.\n",
        "config.enable_cuda_graph = True\n",
        "\n",
        "model = compile(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "kwarg_inputs = dict(\n",
        "    prompt=\n",
        "    '(masterpiece:1,2), best quality, masterpiece, best detailed face, a beautiful girl',\n",
        "    height=512,\n",
        "    width=512,\n",
        "    num_inference_steps=30,\n",
        "    num_images_per_prompt=1,\n",
        ")\n",
        "\n",
        "# NOTE: Warm it up.\n",
        "# The initial calls will trigger compilation and might be very slow.\n",
        "# After that, it should be very fast.\n",
        "for _ in range(3):\n",
        "    output_image = model(**kwarg_inputs).images[0]\n",
        "\n",
        "# Let's see it!\n",
        "# Note: Progress bar might work incorrectly due to the async nature of CUDA.\n",
        "begin = time.time()\n",
        "output_image = model(**kwarg_inputs).images[0]\n",
        "print(f'Inference time: {time.time() - begin:.3f}s')\n",
        "\n",
        "output_image"
      ],
      "metadata": {
        "id": "D3YeEH4Jh59p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}